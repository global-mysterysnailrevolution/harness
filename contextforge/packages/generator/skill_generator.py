"""
Skill Pack Generator — creates SKILL.md folders from AnalysisResult.

Generated skills are deterministic: same AnalysisResult -> same output.
Each skill folder contains:
  - manifest.json (matches skill-pack.schema.json)
  - SKILL.md (the skill content)
  - optional supporting scripts
"""
from __future__ import annotations

import json
import os
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from contextforge.packages.analyzer.analyzer import AnalysisResult
from contextforge.packages.events import emit_event, CorrelationIDs

VERSION = "0.1.0"


def _provenance(result: AnalysisResult) -> dict:
    return {
        "generator": "contextforge",
        "generator_version": VERSION,
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "source_commit": result.source_commit,
        "source_repo": result.repo_path,
        "analysis_hash": result.analysis_hash,
    }


class SkillGenerator:
    """Generates skill packs from repo analysis."""

    def __init__(self, output_dir: str | Path, correlation: Optional[CorrelationIDs] = None):
        self.output_dir = Path(output_dir)
        self.correlation = correlation or CorrelationIDs()

    def generate_all(self, analysis: AnalysisResult) -> list[Path]:
        """Generate all applicable skills for the analyzed repo."""
        skills_created = []

        # Always generate: project-overview
        skills_created.append(self._generate_project_overview(analysis))

        # Conditional: test-runner (if test frameworks detected)
        if analysis.conventions.test_frameworks:
            skills_created.append(self._generate_test_runner(analysis))

        # Conditional: deploy-checker (if Docker or CI detected)
        if analysis.stack.has_docker or analysis.stack.has_ci:
            skills_created.append(self._generate_deploy_checker(analysis))

        # Conditional: dependency-auditor (if package managers detected)
        if analysis.stack.package_managers:
            skills_created.append(self._generate_dependency_auditor(analysis))

        # Conditional: code-conventions (if linters/formatters detected)
        if analysis.conventions.linters or analysis.conventions.formatters:
            skills_created.append(self._generate_code_conventions(analysis))

        return skills_created

    def _write_skill(self, skill_id: str, manifest: dict, skill_md: str,
                     scripts: Optional[list[tuple[str, str]]] = None) -> Path:
        """Write a complete skill pack to disk."""
        skill_dir = self.output_dir / skill_id
        skill_dir.mkdir(parents=True, exist_ok=True)

        # Write manifest
        with open(skill_dir / "manifest.json", "w", encoding="utf-8") as f:
            json.dump(manifest, f, indent=2, ensure_ascii=False)

        # Write SKILL.md with provenance header
        header = (
            f"<!-- Generated by ContextForge v{manifest['provenance']['generator_version']} -->\n"
            f"<!-- Source: {manifest['provenance'].get('source_repo', 'unknown')} -->\n"
            f"<!-- Commit: {manifest['provenance']['source_commit']} -->\n"
            f"<!-- Analysis hash: {manifest['provenance']['analysis_hash']} -->\n"
            f"<!-- Generated at: {manifest['provenance']['generated_at']} -->\n"
            f"<!-- DO NOT EDIT: regenerate with `contextforge generate` -->\n\n"
        )
        with open(skill_dir / "SKILL.md", "w", encoding="utf-8") as f:
            f.write(header + skill_md)

        # Write scripts
        if scripts:
            for script_name, script_content in scripts:
                script_path = skill_dir / script_name
                with open(script_path, "w", encoding="utf-8") as f:
                    f.write(script_content)

        emit_event("generation.skill_created", "generator", self.correlation,
                   payload={"skill_id": skill_id, "path": str(skill_dir)})

        return skill_dir

    # ── Skill generators ────────────────────────────────────────────────

    def _generate_project_overview(self, a: AnalysisResult) -> Path:
        langs = ", ".join(f"{lang} ({count} files)" for lang, count in
                         sorted(a.stack.languages.items(), key=lambda x: -x[1]))
        frameworks = ", ".join(a.stack.frameworks) if a.stack.frameworks else "none detected"
        pkg_mgrs = ", ".join(a.stack.package_managers) if a.stack.package_managers else "none"

        skill_md = f"""# Project Overview

## Stack
- **Languages**: {langs}
- **Frameworks**: {frameworks}
- **Package managers**: {pkg_mgrs}
- **Docker**: {'yes' if a.stack.has_docker else 'no'}
- **CI/CD**: {a.stack.ci_system or 'none detected'}

## Structure
- **Root**: `{a.structure.root}`
- **Total files**: {a.structure.total_files}
- **Key directories**: {', '.join(f'`{d}`' for d in a.structure.key_directories[:15])}
- **Entry points**: {', '.join(f'`{e}`' for e in a.structure.entry_points) or 'none detected'}
- **Config files**: {', '.join(f'`{c}`' for c in a.structure.config_files[:10])}

## Conventions
- **Linters**: {', '.join(a.conventions.linters) or 'none'}
- **Formatters**: {', '.join(a.conventions.formatters) or 'none'}
- **Test frameworks**: {', '.join(a.conventions.test_frameworks) or 'none'}

## Security Notes
- **Environment files present**: {'yes' if a.security.has_env_files else 'no'}
- **Secrets files present**: {'YES - review immediately' if a.security.has_secrets_file else 'no'}
- **Dockerfile present**: {'yes' if a.security.has_dockerfile else 'no'}
- **Exposed ports**: {', '.join(str(p) for p in a.security.exposed_ports) or 'none'}

## Usage
Use this skill as the starting context for any task involving this project.
It provides the foundational understanding of the stack, structure, and conventions.
"""

        manifest = {
            "id": "project-overview",
            "name": "Project Overview",
            "version": "1.0.0",
            "provenance": _provenance(a),
            "description": "Auto-generated project overview including stack, structure, conventions, and security notes. Use as starting context for any task.",
            "triggers": {
                "auto_load": True,
                "keywords": ["project", "overview", "what is this", "codebase"],
            },
            "skill_file": "SKILL.md",
            "tags": ["overview", "context"],
            "security": {"action_classes": ["read"]},
        }

        return self._write_skill("project-overview", manifest, skill_md)

    def _generate_test_runner(self, a: AnalysisResult) -> Path:
        fw_list = ", ".join(a.conventions.test_frameworks)
        commands = []
        for fw in a.conventions.test_frameworks:
            if fw == "pytest":
                commands.append("python3 -m pytest -v")
            elif fw == "jest":
                commands.append("npx jest --verbose")
            elif fw == "vitest":
                commands.append("npx vitest run")

        cmd_block = "\n".join(f"- `{c}`" for c in commands) if commands else "- (configure test command)"

        skill_md = f"""# Test Runner

## Detected Test Frameworks
{fw_list}

## How to Run Tests
{cmd_block}

## Instructions
When asked to run tests, write tests, or verify changes:
1. Identify the relevant test framework from the list above.
2. Run the appropriate command.
3. If tests fail, analyze the output and suggest fixes.
4. For new features, suggest test files following the project's existing test patterns.

## Test File Patterns
Look for existing tests in directories named `tests/`, `test/`, `__tests__/`, or `spec/`.
Follow the naming convention already used in the project.
"""

        manifest = {
            "id": "test-runner",
            "name": "Test Runner",
            "version": "1.0.0",
            "provenance": _provenance(a),
            "description": f"Run and manage tests using {fw_list}. Activated when user mentions testing, verification, or CI.",
            "triggers": {
                "keywords": ["test", "tests", "testing", "verify", "spec", "coverage"],
                "slash_command": "/test",
            },
            "skill_file": "SKILL.md",
            "dependencies": {"tools": ["exec"]},
            "tags": ["testing"],
            "security": {"action_classes": ["read", "exec"]},
        }

        return self._write_skill("test-runner", manifest, skill_md)

    def _generate_deploy_checker(self, a: AnalysisResult) -> Path:
        sections = []
        if a.stack.has_docker:
            ports = ", ".join(str(p) for p in a.security.exposed_ports) or "none detected"
            sections.append(f"## Docker\n- Dockerfile present\n- Exposed ports: {ports}\n")
        if a.stack.has_ci:
            sections.append(f"## CI/CD\n- System: {a.stack.ci_system}\n")

        body = "\n".join(sections)

        skill_md = f"""# Deploy Checker

{body}

## Instructions
When asked about deployment, infrastructure, or CI/CD:
1. Check Dockerfile and docker-compose files for issues.
2. Verify CI configuration is valid.
3. Check for hardcoded secrets or credentials in deploy configs.
4. Verify exposed ports are intentional.
5. Ensure environment variables are properly templated (no defaults with real values).

## Security Checklist
- [ ] No secrets in Dockerfile or CI config
- [ ] Ports exposed are intentional and documented
- [ ] Base images are pinned to specific versions
- [ ] Health checks are configured
- [ ] Resource limits are set (memory, CPU)
"""

        manifest = {
            "id": "deploy-checker",
            "name": "Deploy Checker",
            "version": "1.0.0",
            "provenance": _provenance(a),
            "description": "Checks deployment configs (Docker, CI) for issues, security problems, and best practices.",
            "triggers": {
                "keywords": ["deploy", "deployment", "docker", "ci", "cd", "pipeline", "infrastructure"],
                "file_patterns": ["Dockerfile", "docker-compose*", ".github/workflows/*"],
                "slash_command": "/deploy-check",
            },
            "skill_file": "SKILL.md",
            "tags": ["deployment", "security", "infrastructure"],
            "security": {"action_classes": ["read"]},
        }

        return self._write_skill("deploy-checker", manifest, skill_md)

    def _generate_dependency_auditor(self, a: AnalysisResult) -> Path:
        mgrs = ", ".join(a.stack.package_managers)
        commands = []
        for pm in a.stack.package_managers:
            if pm in ("npm", "yarn", "pnpm"):
                commands.append(f"npm audit --json")
            elif pm == "pip":
                commands.append("pip-audit --format json")
            elif pm == "cargo":
                commands.append("cargo audit")

        cmd_block = "\n".join(f"- `{c}`" for c in commands) if commands else "- (no audit command available)"

        skill_md = f"""# Dependency Auditor

## Package Managers
{mgrs}

## Audit Commands
{cmd_block}

## Instructions
When asked to audit dependencies, check for vulnerabilities, or update packages:
1. Run the audit command for each detected package manager.
2. Parse the JSON output and summarize findings by severity.
3. For critical/high vulnerabilities, suggest specific version bumps.
4. Never auto-update without user approval.
"""

        manifest = {
            "id": "dependency-auditor",
            "name": "Dependency Auditor",
            "version": "1.0.0",
            "provenance": _provenance(a),
            "description": f"Audit dependencies for vulnerabilities using {mgrs}.",
            "triggers": {
                "keywords": ["audit", "vulnerabilities", "dependencies", "outdated", "security scan"],
                "slash_command": "/audit-deps",
            },
            "skill_file": "SKILL.md",
            "dependencies": {"tools": ["exec"]},
            "tags": ["security", "dependencies"],
            "security": {"action_classes": ["read", "exec"]},
        }

        return self._write_skill("dependency-auditor", manifest, skill_md)

    def _generate_code_conventions(self, a: AnalysisResult) -> Path:
        linters = ", ".join(a.conventions.linters) or "none"
        formatters = ", ".join(a.conventions.formatters) or "none"

        skill_md = f"""# Code Conventions

## Linters
{linters}

## Formatters
{formatters}

## Instructions
When writing or reviewing code:
1. Follow the project's existing linter and formatter configurations.
2. Run linters before committing: check the project's scripts or Makefile for lint commands.
3. Do not add new linter rules without discussing with the user first.
4. Match the existing code style in the surrounding files.

## Style Guide
- Indentation: check `.editorconfig` or linter config
- Import ordering: follow the existing convention
- Naming: match the casing style used in surrounding code
"""

        manifest = {
            "id": "code-conventions",
            "name": "Code Conventions",
            "version": "1.0.0",
            "provenance": _provenance(a),
            "description": f"Project coding conventions using {linters} linters and {formatters} formatters.",
            "triggers": {
                "keywords": ["style", "lint", "format", "conventions", "standards"],
                "auto_load": True,
            },
            "skill_file": "SKILL.md",
            "tags": ["conventions", "style"],
            "security": {"action_classes": ["read"]},
        }

        return self._write_skill("code-conventions", manifest, skill_md)
