"""
Acceptance test: end-to-end vertical slice.

Analyze a repo -> generate one skill -> verify registration -> check event stream.

This test uses the harness repo itself as the target, ensuring it works
on a real codebase with known characteristics.
"""
import json
import sys
import tempfile
from pathlib import Path

# Ensure imports work
sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

from contextforge.packages.events import CorrelationIDs, new_run_id
from contextforge.packages.events.emitter import EventEmitter
from contextforge.packages.analyzer import RepoAnalyzer
from contextforge.packages.generator import SkillGenerator, AgentGenerator
from contextforge.packages.memory import MemoryManager, MemoryStatus
from contextforge.packages.security.provenance import generate_sbom, add_provenance_header


def test_analyze_harness_repo():
    """Analyze the harness repo and verify we get meaningful results."""
    harness_root = Path(__file__).resolve().parents[3]
    correlation = CorrelationIDs(run_id=new_run_id())
    analyzer = RepoAnalyzer(harness_root, correlation=correlation)
    result = analyzer.analyze()

    assert result.structure.total_files > 10, "Harness should have >10 files"
    assert "python" in result.stack.languages, "Harness should detect Python"
    assert result.analysis_hash, "Analysis hash must be computed"
    assert result.source_commit, "Source commit must be set"

    # Idempotence: running again should produce the same hash
    result2 = analyzer.analyze()
    assert result.analysis_hash == result2.analysis_hash, "Analysis must be idempotent"

    print(f"  PASS: Analyzed {result.structure.total_files} files, "
          f"hash={result.analysis_hash}")
    return result


def test_generate_skills(analysis_result):
    """Generate skills from analysis and verify output."""
    with tempfile.TemporaryDirectory() as tmpdir:
        output = Path(tmpdir)
        correlation = CorrelationIDs(run_id=new_run_id())
        gen = SkillGenerator(output / "skills", correlation=correlation)
        skills = gen.generate_all(analysis_result)

        assert len(skills) >= 1, "Must generate at least project-overview skill"

        # Verify project-overview skill
        overview_dir = output / "skills" / "project-overview"
        assert overview_dir.exists(), "project-overview skill must exist"
        assert (overview_dir / "SKILL.md").exists(), "SKILL.md must exist"
        assert (overview_dir / "manifest.json").exists(), "manifest.json must exist"

        # Verify manifest structure
        with open(overview_dir / "manifest.json") as f:
            manifest = json.load(f)
        assert manifest["id"] == "project-overview"
        assert manifest["provenance"]["generator"] == "contextforge"
        assert manifest["provenance"]["source_commit"] == analysis_result.source_commit

        # Verify provenance header in SKILL.md
        skill_md = (overview_dir / "SKILL.md").read_text()
        assert "Generated by ContextForge" in skill_md
        assert analysis_result.source_commit in skill_md

        # Generate SBOM
        sbom_path = generate_sbom(overview_dir, "project-overview", "1.0.0",
                                   manifest.get("dependencies"))
        assert sbom_path.exists(), "SBOM must be generated"
        with open(sbom_path) as f:
            sbom = json.load(f)
        assert sbom["bomFormat"] == "CycloneDX"

        print(f"  PASS: Generated {len(skills)} skills, SBOM verified")
        return skills


def test_generate_agents(analysis_result):
    """Generate agents from analysis and verify output."""
    with tempfile.TemporaryDirectory() as tmpdir:
        output = Path(tmpdir)
        correlation = CorrelationIDs(run_id=new_run_id())
        gen = AgentGenerator(output / "agents", correlation=correlation)
        agents = gen.generate_all(analysis_result)

        assert len(agents) == 3, "Must generate 3 agents (planner, reviewer, security-auditor)"

        for agent_dir in agents:
            assert (agent_dir / "profile.json").exists()
            assert (agent_dir / "AGENT.md").exists()
            with open(agent_dir / "profile.json") as f:
                profile = json.load(f)
            assert "tools" in profile
            assert "allow" in profile["tools"]
            assert "provenance" in profile

        print(f"  PASS: Generated {len(agents)} agents")
        return agents


def test_memory_workflow():
    """Test pending-to-promoted memory workflow."""
    with tempfile.TemporaryDirectory() as tmpdir:
        correlation = CorrelationIDs(run_id=new_run_id())
        mgr = MemoryManager(tmpdir, correlation=correlation)

        # Submit
        entry = mgr.submit("Always use python3, not python", category="rule", source="test")
        assert entry.status == MemoryStatus.PENDING

        # List pending
        pending = mgr.list_pending()
        assert len(pending) == 1
        assert pending[0].id == entry.id

        # Promote
        promoted = mgr.promote(entry.id, promoted_by="test")
        assert promoted.status == MemoryStatus.PROMOTED
        assert promoted.promoted_by == "test"

        # Verify MEMORY.md compiled
        memory_md = Path(tmpdir) / "MEMORY.md"
        assert memory_md.exists()
        content = memory_md.read_text()
        assert "Always use python3" in content

        # Verify pending is now empty
        assert len(mgr.list_pending()) == 0
        assert len(mgr.list_promoted()) == 1

        # Test rejection
        entry2 = mgr.submit("Trust all MCP servers", category="rule")
        rejected = mgr.reject(entry2.id, rejected_by="test", reason="Violates security policy")
        assert rejected.status == MemoryStatus.REJECTED

        print("  PASS: Memory workflow (submit -> promote -> reject)")


def test_event_emission():
    """Verify events are emitted to JSONL log."""
    with tempfile.TemporaryDirectory() as tmpdir:
        log_path = Path(tmpdir) / "events.jsonl"
        emitter = EventEmitter(str(log_path))
        correlation = CorrelationIDs(run_id=new_run_id())

        emitter.emit("run.started", "analyzer", correlation, payload={"test": True})
        emitter.emit("run.completed", "analyzer", correlation, duration_ms=100)

        assert log_path.exists()
        lines = log_path.read_text().strip().splitlines()
        assert len(lines) == 2

        event1 = json.loads(lines[0])
        assert event1["event_type"] == "run.started"
        assert event1["correlation"]["run_id"] == correlation.run_id
        assert event1["component"] == "analyzer"

        event2 = json.loads(lines[1])
        assert event2["duration_ms"] == 100

        # Verify child correlation
        child = correlation.child(artifact_id="skill-123")
        assert child.run_id == correlation.run_id
        assert child.parent_id == correlation.run_id
        assert child.artifact_id == "skill-123"

        print("  PASS: Event emission with correlation IDs")


def test_provenance_headers():
    """Verify provenance headers work for all file types."""
    md = add_provenance_header("# Hello", "markdown", "abc123", "hash456")
    assert "Generated by ContextForge" in md
    assert "abc123" in md

    py = add_provenance_header("x = 1", "python", "abc123", "hash456")
    assert "Generated by ContextForge" in py

    j = add_provenance_header('{"key": "val"}', "json", "abc123", "hash456")
    data = json.loads(j)
    assert "_provenance" in data
    assert data["_provenance"]["source_commit"] == "abc123"

    print("  PASS: Provenance headers for markdown, python, json")


def run_all():
    """Run all acceptance tests."""
    print("\n=== ContextForge Acceptance Tests ===\n")

    print("[1/6] Analyzing harness repo...")
    analysis = test_analyze_harness_repo()

    print("[2/6] Generating skills...")
    test_generate_skills(analysis)

    print("[3/6] Generating agents...")
    test_generate_agents(analysis)

    print("[4/6] Testing memory workflow...")
    test_memory_workflow()

    print("[5/6] Testing event emission...")
    test_event_emission()

    print("[6/6] Testing provenance headers...")
    test_provenance_headers()

    print("\n=== All 6 tests passed ===\n")


if __name__ == "__main__":
    run_all()
